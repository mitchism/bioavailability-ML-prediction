{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSM VM config prep\n",
    "import findspark\n",
    "findspark.init('/home/mitch/spark-3.3.0-bin-hadoop2')\n",
    "import pyspark\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BApredsV3').getOrCreate()\n",
    "\n",
    "# --- suppress future spark warnings/error/etc output ---\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data_and_merge():\n",
    "    labels_and_calcs = spark.read.csv(\"bioavailability_data_final.csv\",inferSchema=True,sep=',',header=True)\n",
    "    df1 = labels_and_calcs.toPandas()\n",
    "\n",
    "    df2 = pd.read_pickle('bioavailabilityData_w_Frags_simpler.pkl')\n",
    "    df2 = df2.drop(columns=['drug_smiles','ba_pct'])\n",
    "\n",
    "    data = pd.merge(df1,df2,how='left',left_on='_c0',right_on=df2.index)\n",
    "    data = spark.createDataFrame(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_data_and_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eb87e",
   "metadata": {},
   "source": [
    "* Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# INTIAL LABELS:\n",
    "# --- Data has 1 continuous label column, and 4 categorical label columns (discretized variants of continuous label).\n",
    "# ------ categorical labels applied by dividing the continuous label values into 3-5 categories \n",
    "# ------ the value range associated with each group were selected based on histogram dist./mean/stdev\n",
    "# --- We'll add one more discretization variant,  using Spark's built-in QuantileDiscretizer\n",
    "'''\n",
    "# -- Add QuantileDiscretizer labels\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "import pandas as pd\n",
    "qd5 = QuantileDiscretizer(numBuckets=5,inputCol='BA_pct',outputCol='label_QD5')\n",
    "\n",
    "data_wLabels = qd5.fit(data).transform(data)\n",
    "\n",
    "# -- INDEX / ENCODE LABELS\n",
    "from pyspark.ml.feature import (StringIndexer,OneHotEncoder)\n",
    "\n",
    "label_quant0 = 'BA_pct'\n",
    "label_cat0_vector = OneHotEncoder(inputCol='label_QD5',outputCol='label_cat0_vector')\n",
    "\n",
    "label_cat1_index = StringIndexer(inputCol='label1',outputCol='label_cat1_index')\n",
    "label_cat1_vector = OneHotEncoder(inputCol='label_cat1_index',outputCol='label_cat1_vector')\n",
    "\n",
    "label_cat2_index = StringIndexer(inputCol='label2',outputCol='label_cat2_index')\n",
    "label_cat2_vector = OneHotEncoder(inputCol='label_cat2_index',outputCol='label_cat2_vector')\n",
    "\n",
    "label_cat3_index = StringIndexer(inputCol='label3a',outputCol='label_cat3_index')\n",
    "label_cat3_vector = OneHotEncoder(inputCol='label_cat3_index',outputCol='label_cat3_vector')\n",
    "\n",
    "label_cat4_index = StringIndexer(inputCol='label3b',outputCol='label_cat4_index')\n",
    "label_cat4_vector = OneHotEncoder(inputCol='label_cat4_index',outputCol='label_cat4_vector')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "label_pipeline = Pipeline(stages=[label_cat0_vector,\n",
    "                                 label_cat1_index,label_cat1_vector,\n",
    "                                 label_cat2_index,label_cat2_vector,\n",
    "                                 label_cat3_index,label_cat3_vector,\n",
    "                                 label_cat4_index,label_cat4_vector])\n",
    "data_wLabels = label_pipeline.fit(data_wLabels).transform(data_wLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram,Word2Vec,CountVectorizer,HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # Fragments NLP processing\n",
    "'''\n",
    "from pyspark.ml.feature import NGram,Word2Vec,CountVectorizer,HashingTF,IDF\n",
    "\n",
    "frag_type = 'frags_all'\n",
    "frag_type_short = frag_type.replace('_all','')\n",
    "\n",
    "cv = CountVectorizer(inputCol=frag_type, outputCol=f\"{frag_type_short}_cv\", minDF=2.0)\n",
    "cv_idf = IDF(inputCol=f\"{frag_type_short}_cv\", outputCol=f\"{frag_type_short}_cv_idf\")\n",
    "\n",
    "tf = HashingTF(inputCol=frag_type, outputCol=f\"{frag_type_short}_tf\", numFeatures=2867)\n",
    "tf_idf = IDF(inputCol=f\"{frag_type_short}_tf\", outputCol=f\"{frag_type_short}_tf_idf\")\n",
    "\n",
    "w2v = Word2Vec(inputCol=frag_type, outputCol=f\"{frag_type_short}_w2v\")\n",
    "\n",
    "n2gram = NGram(n=2, inputCol=frag_type, outputCol=f\"{frag_type_short}_n2g\")\n",
    "n2gram_cv = CountVectorizer(inputCol=f\"{frag_type_short}_n2g\", outputCol=f\"{frag_type_short}_n2g_cv\", minDF=2.0)\n",
    "n2gram_cv_idf = IDF(inputCol=f\"{frag_type_short}_n2g_cv\", outputCol=f\"{frag_type_short}_n2g_cv_idf\")\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[cv,cv_idf,tf,tf_idf,w2v,n2gram,n2gram_cv,n2gram_cv_idf])\n",
    "\n",
    "data_wLabels_NLP = nlp_pipeline.fit(data_wLabels).transform(data_wLabels)\n",
    "\n",
    "data_wLabels_NLP.toPandas().to_pickle(\"data_wLabels_NLP_aspandas.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236b294",
   "metadata": {},
   "source": [
    "* make fragment NLP feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d809e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer)\n",
    "\n",
    "vector_assemblers = []\n",
    "\n",
    "alternative_features = ['frags_cv', 'frags_cv_idf', 'frags_tf', 'frags_tf_idf', \n",
    "                        'frags_w2v', 'frags_n2g_cv', 'frags_n2g_cv_idf']\n",
    "output_features = \"\"\n",
    "for feats in alternative_features:\n",
    "    \n",
    "    feats_input = [feats]\n",
    "    feats_output = f\"FEAT_{feats}\"\n",
    "    \n",
    "    vec_assembler = VectorAssembler(inputCols=feats_input, outputCol=feats_output)\n",
    "    \n",
    "    vector_assemblers.append(vec_assembler)\n",
    "    \n",
    "    output_features += \"'\"+feats_output+\"'\"+\", \"\n",
    "\n",
    "output_features = output_features[0:len(output_features)-2]\n",
    "print(output_features)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "feature_pipeline = Pipeline(stages=[x for x in vector_assemblers])\n",
    "\n",
    "data_wLabels_NLPFeatures = feature_pipeline.fit(data_wLabels_NLP).transform(data_wLabels_NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424af645",
   "metadata": {},
   "source": [
    "* export data w/ NLP vector features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wLabels_NLPFeatures.toPandas().to_pickle(\"data_wLabels_NLPFeatures_aspandas.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04847a22",
   "metadata": {},
   "source": [
    "* Prepare Vector Features for RDKit calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39064405",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# RDKit \"1D\" FEATURE SELECTION:\n",
    "'''\n",
    "# to load the Features Information, use the command:\n",
    "featuresDF = pd.read_parquet('featuresCatalogDF.parquet')\n",
    "\n",
    "#index_pos = featuresDF[featuresDF['name']=='F1a'].index[0]\n",
    "feature_set1a = featuresDF.loc[0,'features']\n",
    "feature_set1b = featuresDF.loc[1,'features']\n",
    "feature_set2a = featuresDF.loc[2,'features']\n",
    "feature_set2b = featuresDF.loc[3,'features']\n",
    "feature_set3  = featuresDF.loc[4,'features']\n",
    "feature_set4a = featuresDF.loc[5,'features']\n",
    "feature_set4b = featuresDF.loc[6,'features']\n",
    "F1bANOVA = featuresDF.loc[7,'features']\n",
    "F2bANOVA = featuresDF.loc[8,'features']\n",
    "\n",
    "# VECTOR ASSEMBLY - feature sets 1a,1b,2a,2b,3,4a,4b\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer)\n",
    "\n",
    "vec_assembler1a = VectorAssembler(inputCols = feature_set1a, outputCol='FEAT_rdkit_1a')\n",
    "vec_assembler1b = VectorAssembler(inputCols = feature_set1b, outputCol='FEAT_rdkit_1b')\n",
    "vec_assembler2a = VectorAssembler(inputCols = feature_set2a, outputCol='FEAT_rdkit_2a')\n",
    "vec_assembler2b = VectorAssembler(inputCols = feature_set2b, outputCol='FEAT_rdkit_2b')\n",
    "vec_assembler3 = VectorAssembler(inputCols = feature_set3, outputCol='FEAT_rdkit_3')\n",
    "vec_assembler4a = VectorAssembler(inputCols = feature_set4a, outputCol='FEAT_rdkit_4a')\n",
    "vec_assembler4b = VectorAssembler(inputCols = feature_set4b, outputCol='FEAT_rdkit_4b')\n",
    "vec_assembler1bANOVA = VectorAssembler(inputCols = F1bANOVA, outputCol='FEAT_rdkit_1bANOVA')\n",
    "vec_assembler2bANOVA = VectorAssembler(inputCols = F2bANOVA, outputCol='FEAT_rdkit_2bANOVA')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "feature_pipeline = Pipeline(stages=[vec_assembler1a,\n",
    "                                    vec_assembler1b,\n",
    "                                    vec_assembler2a,\n",
    "                                    vec_assembler2b,\n",
    "                                    vec_assembler3,\n",
    "                                    vec_assembler4a,\n",
    "                                    vec_assembler4b,\n",
    "                                    vec_assembler1bANOVA,\n",
    "                                   vec_assembler2bANOVA])\n",
    "data_features = feature_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9272770",
   "metadata": {},
   "source": [
    "* clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data_features.drop('label1')\n",
    "data_features = data_features.drop('label2')\n",
    "data_features = data_features.drop('label3a')\n",
    "data_features = data_features.drop('label3b')\n",
    "data_features = data_features.drop('label_cat0_vector')\n",
    "data_features = data_features.drop('label_cat1_vector')\n",
    "data_features = data_features.drop('label_cat2_vector')\n",
    "data_features = data_features.drop('label_cat3_vector')\n",
    "data_features = data_features.drop('label_cat4_vector')\n",
    "\n",
    "data_features = data_features.withColumnRenamed('BA_pct','label_q0')\n",
    "data_features = data_features.withColumnRenamed('label_QD5','label_cat0')\n",
    "data_features = data_features.withColumnRenamed('label_cat1_index','label_cat1')\n",
    "data_features = data_features.withColumnRenamed('label_cat2_index','label_cat2')\n",
    "data_features = data_features.withColumnRenamed('label_cat3_index','label_cat3')\n",
    "data_features = data_features.withColumnRenamed('label_cat4_index','label_cat4')\n",
    "\n",
    "\n",
    "features_to_drop = [\n",
    "    'PEOE_VSA1','PEOE_VSA2','PEOE_VSA3','PEOE_VSA4','PEOE_VSA5','PEOE_VSA6','PEOE_VSA7','PEOE_VSA8',\n",
    "    'PEOE_VSA9','PEOE_VSA10','PEOE_VSA11','PEOE_VSA12','PEOE_VSA13','PEOE_VSA14','SMR_VSA1','SMR_VSA2',\n",
    "    'SMR_VSA3','SMR_VSA4','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA8','SMR_VSA9','SMR_VSA10','SlogP_VSA1',\n",
    "    'SlogP_VSA2','SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7','SlogP_VSA8','SlogP_VSA9',\n",
    "    'SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','PEOE_VSA1.1','PEOE_VSA2.1','PEOE_VSA3.1','PEOE_VSA4.1',\n",
    "    'PEOE_VSA5.1','PEOE_VSA6.1','PEOE_VSA7.1','PEOE_VSA8.1','PEOE_VSA9.1','PEOE_VSA10.1','PEOE_VSA11.1',\n",
    "    'PEOE_VSA12.1','PEOE_VSA13.1','PEOE_VSA14.1','SMR_VSA1.1','SMR_VSA2.1','SMR_VSA3.1','SMR_VSA4.1',\n",
    "    'SMR_VSA5.1','SMR_VSA6.1','SMR_VSA7.1','SMR_VSA8.1','SMR_VSA9.1','SMR_VSA10.1','SlogP_VSA1.1',\n",
    "    'SlogP_VSA2.1','SlogP_VSA3.1','SlogP_VSA4.1','SlogP_VSA5.1','SlogP_VSA6.1','SlogP_VSA7.1','SlogP_VSA8.1',\n",
    "    'SlogP_VSA9.1','SlogP_VSA10.1','SlogP_VSA11.1','SlogP_VSA12.1']\n",
    "for x in features_to_drop:\n",
    "    data_features = data_features.drop(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d45411",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_column_order = [\n",
    "    '_c0','Name','drug_name', \n",
    "    'label_q0','label_cat0','label_cat1','label_cat2','label_cat3','label_cat4',\n",
    "    'smile','MolWt','ExactMolWt','qed','MolLogP','MolMR','VSA_total','LabuteASA',\n",
    "    'TPSA','MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge','MinAbsPartialCharge','NumHAcceptors',\n",
    "    'NumHDonors','HeavyAtomCount','NumHeteroatoms','NumRotatableBonds','NHOHCount','NOCount','FractionCSP3',\n",
    "    'RingCount','NumAliphaticRings','NumAromaticRings','NumAliphaticHeterocycles','NumAromaticHeterocycles',\n",
    "    'NumSaturatedHeterocycles','NumSaturatedRings','BalabanJ','BertzCT','HallKierAlpha','fracVSA_PEOE01',\n",
    "    'fracVSA_PEOE02','fracVSA_PEOE03','fracVSA_PEOE04','fracVSA_PEOE05','fracVSA_PEOE06','fracVSA_PEOE07',\n",
    "    'fracVSA_PEOE08','fracVSA_PEOE09','fracVSA_PEOE10','fracVSA_PEOE11','fracVSA_PEOE12','fracVSA_PEOE13',\n",
    "    'fracVSA_PEOE14','fracVSA_SMR01','fracVSA_SMR02','fracVSA_SMR03','fracVSA_SMR04','fracVSA_SMR05',\n",
    "    'fracVSA_SMR06','fracVSA_SMR07','fracVSA_SMR08','fracVSA_SMR09','fracVSA_SMR10','fracVSA_SlogP01',\n",
    "    'fracVSA_SlogP02','fracVSA_SlogP03','fracVSA_SlogP04','fracVSA_SlogP05','fracVSA_SlogP06','fracVSA_SlogP07',\n",
    "    'fracVSA_SlogP08','fracVSA_SlogP09','fracVSA_SlogP10','fracVSA_SlogP11','fracVSA_SlogP12',\n",
    "    'FEAT_rdkit_1a','FEAT_rdkit_1b','FEAT_rdkit_2a','FEAT_rdkit_2b','FEAT_rdkit_3','FEAT_rdkit_4a',\n",
    "    'FEAT_rdkit_4b','FEAT_rdkit_1bANOVA','FEAT_rdkit_2bANOVA',\n",
    "    'frags_all','frags_better','frags_best','frags_efgs','frags_brics',\n",
    "    'frags_cv','frags_cv_idf','frags_tf','frags_tf_idf','frags_w2v','frags_n2g','frags_n2g_cv','frags_n2g_cv_idf',\n",
    "    'FEAT_frags_cv','FEAT_frags_cv_idf','FEAT_frags_tf','FEAT_frags_tf_idf',\n",
    "    'FEAT_frags_w2v','FEAT_frags_n2g_cv','FEAT_frags_n2g_cv_idf']\n",
    "\n",
    "data_features_clean = data_features.select(final_column_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850f035",
   "metadata": {},
   "source": [
    "* export cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a97805",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_clean.toPandas().to_pickle(\"data_final_NEW_2022-09-05_aspandas.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
