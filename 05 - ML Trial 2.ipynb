{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0094196c",
   "metadata": {},
   "source": [
    "# ML Trial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc7aa1",
   "metadata": {},
   "source": [
    "### <font color='blue'> Test the attempted improvements in feature selection </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef811f",
   "metadata": {},
   "source": [
    "* Recall, in the last notebook we did some feature engineering to provide the following feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresDF = pd.read_parquet('featuresCatalogDF_2022-08-17_new.parquet')\n",
    "featuresDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73efb86",
   "metadata": {},
   "source": [
    "1. With all of the new feature sets saved in the **features catalog**, we can load the original data set and run through the data processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSM VM config prep\n",
    "import findspark\n",
    "findspark.init('/home/mitch/spark-3.3.0-bin-hadoop2')\n",
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BApredsV1').getOrCreate()\n",
    "\n",
    "# load the data\n",
    "data = spark.read.csv(\"bioavailability_data_final.csv\",inferSchema=True,sep=',',header=True)\n",
    "\n",
    "''' # FEATURE SELECTION:\n",
    "'''\n",
    "featuresDF = pd.read_parquet('featuresCatalogDF_2022-08-17_new.parquet')\n",
    "feature_set1a = featuresDF.loc[0,'features']\n",
    "feature_set1b = featuresDF.loc[1,'features']\n",
    "feature_set2a = featuresDF.loc[2,'features']\n",
    "feature_set2b = featuresDF.loc[3,'features']\n",
    "feature_set3  = featuresDF.loc[4,'features']\n",
    "feature_set4a = featuresDF.loc[5,'features']\n",
    "feature_set4b = featuresDF.loc[6,'features']\n",
    "F1bANOVA = featuresDF.loc[7,'features']\n",
    "F2bANOVA = featuresDF.loc[8,'features']\n",
    "\n",
    "''' # VECTORIZE FEATURES\n",
    "'''\n",
    "# VECTOR ASSEMBLY - feature sets 1a,1b,2a,2b,3,4a,4b\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer)\n",
    "\n",
    "vec_assembler1a = VectorAssembler(inputCols = feature_set1a, outputCol='features1a')\n",
    "vec_assembler1b = VectorAssembler(inputCols = feature_set1b, outputCol='features1b')\n",
    "vec_assembler2a = VectorAssembler(inputCols = feature_set2a, outputCol='features2a')\n",
    "vec_assembler2b = VectorAssembler(inputCols = feature_set2b, outputCol='features2b')\n",
    "vec_assembler3 = VectorAssembler(inputCols = feature_set3, outputCol='features3')\n",
    "vec_assembler4a = VectorAssembler(inputCols = feature_set4a, outputCol='features4a')\n",
    "vec_assembler4b = VectorAssembler(inputCols = feature_set4b, outputCol='features4b')\n",
    "vec_assembler1bANOVA = VectorAssembler(inputCols = F1bANOVA, outputCol='F1bANOVA')\n",
    "vec_assembler2bANOVA = VectorAssembler(inputCols = F2bANOVA, outputCol='F2bANOVA')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "feature_pipeline = Pipeline(stages=[vec_assembler1a,\n",
    "                                    vec_assembler1b,\n",
    "                                    vec_assembler2a,\n",
    "                                    vec_assembler2b,\n",
    "                                    vec_assembler3,\n",
    "                                    vec_assembler4a,\n",
    "                                    vec_assembler4b,\n",
    "                                    vec_assembler1bANOVA,\n",
    "                                   vec_assembler2bANOVA])\n",
    "data_features = feature_pipeline.fit(data).transform(data)\n",
    "\n",
    "\n",
    "''' # DEPENDENT VARIABLE LABELS \n",
    "'''\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "import pandas as pd\n",
    "qd5 = QuantileDiscretizer(numBuckets=5,inputCol='BA_pct',outputCol='label_QD5')\n",
    "\n",
    "data_features = qd5.fit(data_features).transform(data_features)\n",
    "\n",
    "# -- INDEX / ENCODE LABELS\n",
    "from pyspark.ml.feature import (StringIndexer,OneHotEncoder)\n",
    "\n",
    "label_quant0 = 'BA_pct'\n",
    "label_cat0_vector = OneHotEncoder(inputCol='label_QD5',outputCol='label_cat0_vector')\n",
    "\n",
    "label_cat1_index = StringIndexer(inputCol='label1',outputCol='label_cat1_index')\n",
    "label_cat1_vector = OneHotEncoder(inputCol='label_cat1_index',outputCol='label_cat1_vector')\n",
    "\n",
    "label_cat2_index = StringIndexer(inputCol='label2',outputCol='label_cat2_index')\n",
    "label_cat2_vector = OneHotEncoder(inputCol='label_cat2_index',outputCol='label_cat2_vector')\n",
    "\n",
    "label_cat3_index = StringIndexer(inputCol='label3a',outputCol='label_cat3_index')\n",
    "label_cat3_vector = OneHotEncoder(inputCol='label_cat3_index',outputCol='label_cat3_vector')\n",
    "\n",
    "label_cat4_index = StringIndexer(inputCol='label3b',outputCol='label_cat4_index')\n",
    "label_cat4_vector = OneHotEncoder(inputCol='label_cat4_index',outputCol='label_cat4_vector')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "label_pipeline = Pipeline(stages=[label_cat0_vector,\n",
    "                                 label_cat1_index,label_cat1_vector,\n",
    "                                 label_cat2_index,label_cat2_vector,\n",
    "                                 label_cat3_index,label_cat3_vector,\n",
    "                                 label_cat4_index,label_cat4_vector])\n",
    "\n",
    "data_features = data_features.select(['Name','BA_pct',\n",
    "                                      'label_QD5','label1','label2','label3a','label3b',\n",
    "                                      'features1a','features1b','features2a','features2b',\n",
    "                                      'features3','features4a','features4b','F1bANOVA','F2bANOVA'])\n",
    "\n",
    "data_prefinal = label_pipeline.fit(data_features).transform(data_features)\n",
    "\n",
    "data_prefinal2 = data_prefinal.withColumnRenamed('BA_pct','label_q0')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_QD5','label_cat0')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat1_index','label_cat1')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat2_index','label_cat2')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label3a','label3')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label3b','label4')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat3_index','label_cat3')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat4_index','label_cat4')\n",
    "\n",
    "data_final = data_prefinal2.select(['Name',\n",
    "                                    'label_q0',\n",
    "                                    'label_cat0','label_cat1',\n",
    "                                    'label_cat2','label_cat3','label_cat4',\n",
    "                                    'features1a','features1b','features2a','features2b',\n",
    "                                    'features3','features4a','features4b',\n",
    "                                    'F1bANOVA','F2bANOVA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5eefb",
   "metadata": {},
   "source": [
    "2. let's also check if **Feature Scaling** has any effect; let's add that in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # TEST FEATURE SCALING\n",
    "'''\n",
    "# Scale values\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler1 = StandardScaler(inputCol=\"features2b\", outputCol=\"features2bSs\", withStd=True, withMean=False)\n",
    "scaler2 = StandardScaler(inputCol=\"features2b\", outputCol=\"features2bSm\", withStd=False, withMean=True)\n",
    "\n",
    "data_final = scaler1.fit(data_final).transform(data_final)\n",
    "data_final = scaler2.fit(data_final).transform(data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a8da0",
   "metadata": {},
   "source": [
    "<font color='gray'> ..._now, all of the different labels and feature sets have been added to the data._ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5bd10",
   "metadata": {},
   "source": [
    "3. Let's **backup the dataset** so we can avoid executing all of the above data prep again in a new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1cd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.toPandas().to_pickle(\"bioavailability_data_final_withLabelsAndFeaturesV1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1587fba",
   "metadata": {},
   "source": [
    "4. Now, we can easily load the processed data and get right into ML testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data_final = pd.read_pickle(\"bioavailability_data_final_withLabelsAndFeaturesV1.pkl\")\n",
    "data_final = spark.createDataFrame(data_final)\n",
    "\n",
    "''' \n",
    "# RUN ML COMPARISON ACROSS ALL FEATURE SETS\n",
    "'''\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "labelName = 'label_q0'  # SPECIFY\n",
    "lr_df = pd.DataFrame()\n",
    "df = lr_df\n",
    "\n",
    "allFeatures = ['features1a','features1b',\n",
    "               'features2a','features2b',\n",
    "               'features3','features4a','features4b',\n",
    "               'F1bANOVA','F2bANOVA',\"features2bSs\",\"features2bSm\"]\n",
    "\n",
    "subset = data_final.select(['Name',labelName,\n",
    "                            'features1a','features1b',\n",
    "                            'features2a','features2b',\n",
    "                            'features3','features4a','features4b',\n",
    "                            'F1bANOVA','F2bANOVA',\"features2bSs\",\"features2bSm\"]) \n",
    "train,test = subset.randomSplit([0.7,0.3])\n",
    "\n",
    "#coefficients = {}\n",
    "i = 1\n",
    "for index,features in enumerate(allFeatures):\n",
    "    featnum = ['F1a','F1b','F2a','F2b','F3','F4a','F4b','F1bANOVA','F2bANOVA','F2bSs','F2bSm']\n",
    "    featuresName = features\n",
    "    \n",
    "    lr = LinearRegression(featuresCol=featuresName,labelCol=labelName,predictionCol='prediction')\n",
    "    \n",
    "    ''' # SPECIFY MODEL \n",
    "    '''\n",
    "    modeltype = lr \n",
    "    modeltypeVariantNo = '1' \n",
    "    modelname = f\"lr{modeltypeVariantNo}_{featnum[index]}\"  \n",
    "    \n",
    "    # FIT/TRAIN MODEL & TRANSFORM DATA\n",
    "    mymodel = modeltype.fit(train)\n",
    "    myresults = mymodel.transform(test)\n",
    "    \n",
    "    '''\n",
    "    # Save predictions\n",
    "    export = myresults.select(['Name','prediction'])\n",
    "    export = export.withColumnRenamed('prediction',f'pred_{modelname}')\n",
    "    export = export.withColumnRenamed('Name','Name2')\n",
    "    if index == 0:\n",
    "        comparedPredictions = data_final.join(export,data_final.Name == export.Name2,how=\"leftouter\")\n",
    "    else:\n",
    "        comparedPredictions = comparedPredictions.join(export,comparedPredictions.Name == export.Name2,how=\"leftouter\")\n",
    "    '''\n",
    "    \n",
    "    # CALCULATE KEY EVALS\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    regEvaluator = RegressionEvaluator(labelCol=labelName,predictionCol='prediction')\n",
    "    evalMetrics = {regEvaluator:['rmse','mse','mae','r2','var']}\n",
    "    evaluation2 = []\n",
    "    evaluator = regEvaluator\n",
    "    for each_metric in evalMetrics[evaluator]:        \n",
    "        metric = each_metric\n",
    "        result = evaluator.evaluate(myresults, {evaluator.metricName: metric})\n",
    "        evaluation2.append((metric,result))\n",
    "    r2_adj = mymodel.summary.r2adj\n",
    "    evaluation2.append(('r2_adj',r2_adj))\n",
    "    column0 = [x for x,y in evaluation2]\n",
    "    column1 = [y for x,y in evaluation2]\n",
    "    lr_df['metric'] = column0\n",
    "    lr_df[modelname] = column1\n",
    "    \n",
    "    '''\n",
    "    # save coefficients for each LR model\n",
    "    for k, v in data_final.schema[featuresName].metadata[\"ml_attr\"][\"attrs\"].items():\n",
    "        coefficients_df = pd.DataFrame(v)\n",
    "        # print coefficient and intercept\n",
    "        #print(mymodel.coefficients, mymodel.intercept)\n",
    "        coefficients_df[modelname] = mymodel.coefficients\n",
    "        #coefficients.append(coefficients_df)\n",
    "        coefficients[i] = coefficients_df\n",
    "    '''\n",
    "    i += 1\n",
    "\n",
    "# combine all coefficient tables into 1 df\n",
    "'''\n",
    "for key in range (1,8):\n",
    "    if key == 1:\n",
    "        dfx = coefficients[key]\n",
    "    elif key > 1:\n",
    "        dfx = pd.merge(dfx,coefficients[key],on='name',how='left').fillna(' ')\n",
    "    coefficients_vs_features = dfx\n",
    "'''\n",
    "lr_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270624f0",
   "metadata": {},
   "source": [
    "<font color='gray'> _for easier comparison of feature sets, we can transpose the Eval DF and sort by r2 value:_ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c917bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best features in terms of r2: \n",
    "testx = lr_df.set_index('metric')\n",
    "testx = testx.transpose()\n",
    "testx.sort_values(by=['r2'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a24a96",
   "metadata": {},
   "source": [
    "<font color='red'> If this part above worked: </font>\n",
    "* delete the commented code blocks above, as they're not needed\n",
    "* delete the code below which seems messier "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0cd1bef",
   "metadata": {},
   "source": [
    "finalColumns = ['Name','label_q0','features2b','features2bSs','features2bSm','F1bANOVA','F2bANOVA','features1a','features1b','features3','features4a','features4b']\n",
    "featuresName = '' # temp value, redefined below\n",
    "lr_df = pd.DataFrame() # already exists from prior run\n",
    "labelName = 'label_q0'  # SPECIFY\n",
    "df = lr_df\n",
    "\n",
    "subset = data_final.select(finalColumns) \n",
    "train,test1,test2 = subset.randomSplit([0.75,0.125,0.125])\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "allFeatures = finalColumns\n",
    "allFeatures.remove('Name')\n",
    "allFeatures.remove(labelName)\n",
    "i = 1\n",
    "coefficients = {}\n",
    "for index,features in enumerate(allFeatures):\n",
    "    featnum = allFeatures\n",
    "    featuresName = features\n",
    "    \n",
    "    lr = LinearRegression(featuresCol=featuresName,labelCol=labelName,predictionCol='prediction')\n",
    "    \n",
    "    '''# SPECIFY MODEL \n",
    "    '''\n",
    "    modeltype = lr  # SPECIFY (lr,dtr,rfr,gbtr,glr,ir)\n",
    "    modeltypeVariantNo = '1' # SPECIFY modeltype variation number\n",
    "    modelname = f\"lr{modeltypeVariantNo}_{featnum[index]}\"  \n",
    "    \n",
    "    # FIT/TRAIN MODEL & TRANSFORM DATA\n",
    "    mymodel = modeltype.fit(train)\n",
    "    \n",
    "    myresults1 = mymodel.transform(test1)\n",
    "    if i == 1:\n",
    "        myresults1 = myresults1.withColumnRenamed('Name','name')\n",
    "        myresults2 = mymodel.transform(test2)\n",
    "        myresults2 = myresults2.withColumnRenamed('Name','delete_me')\n",
    "    \n",
    "        myresults = myresults1.join(myresults2,myresults1.name == myresults2.delete_me)\n",
    "    else:\n",
    "        myresults1 = myresults1.withColumnRenamed('Name','namex')\n",
    "        myresults2 = mymodel.transform(test2)\n",
    "        myresults2 = myresults2.withColumnRenamed('Name','delete_me')\n",
    "    \n",
    "        myresults = myresults1.join(myresults2,myresults1.namex == myresults2.delete_me)\n",
    "    \n",
    "    \n",
    "    myresults = myresults.drop('delete_me')\n",
    "    myresults = myresults.drop('namex')\n",
    "    '''\n",
    "    # Save predictions\n",
    "    export = myresults.select(['Name','prediction'])\n",
    "    export = export.withColumnRenamed('prediction',f'pred_{modelname}')\n",
    "    export = export.withColumnRenamed('Name','Name2')\n",
    "    if index == 0:\n",
    "        comparedPredictions = data_final.join(export,data_final.Name == export.Name2,how=\"leftouter\")\n",
    "    else:\n",
    "        comparedPredictions = comparedPredictions.join(export,comparedPredictions.Name == export.Name2,how=\"leftouter\")\n",
    "    '''\n",
    "    \n",
    "    # CALCULATE KEY EVALS\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    regEvaluator = RegressionEvaluator(labelCol=labelName,predictionCol='prediction')\n",
    "    evalMetrics = {regEvaluator:['rmse','mse','mae','r2','var']}\n",
    "    evaluation2 = []\n",
    "    evaluator = regEvaluator\n",
    "    for each_metric in evalMetrics[evaluator]:        \n",
    "        metric = each_metric\n",
    "        \n",
    "        result1 = evaluator.evaluate(myresults1, {evaluator.metricName: metric})\n",
    "        result2 = evaluator.evaluate(myresults2, {evaluator.metricName: metric})\n",
    "        result_avg = ((result1+result2)/2)\n",
    "        \n",
    "        evaluation2.append((metric,result_avg))\n",
    "    r2_adj = mymodel.summary.r2adj\n",
    "    evaluation2.append(('r2_adj(Training)',r2_adj))\n",
    "    column0 = [x for x,y in evaluation2]\n",
    "    column1 = [y for x,y in evaluation2]\n",
    "    lr_df['metric'] = column0\n",
    "    lr_df[modelname] = column1\n",
    "    '''\n",
    "    # save coefficients for each LR model\n",
    "    for k, v in data_final.schema[featuresName].metadata[\"ml_attr\"][\"attrs\"].items():\n",
    "        coefficients_df = pd.DataFrame(v)\n",
    "        coefficients_df[modelname] = mymodel.coefficients\n",
    "        coefficients[i] = coefficients_df\n",
    "    '''\n",
    "    i += 1\n",
    "'''\n",
    "# combine all coefficient tables into 1 df\n",
    "for key in range (1,8):\n",
    "    if key == 1:\n",
    "        dfx = coefficients[key]\n",
    "    elif key > 1:\n",
    "        dfx = pd.merge(dfx,coefficients[key],on='name',how='left').fillna(' ')\n",
    "    coefficients_vs_features = dfx\n",
    "'''\n",
    "# skipped coefficients table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94500224",
   "metadata": {},
   "source": [
    "### Investigate ML model Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fb9ee",
   "metadata": {},
   "source": [
    "* ML hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e262394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5791a07",
   "metadata": {},
   "source": [
    "we will optimize hyperparameters such as maxIter, and regularization method\n",
    "* Regularization by Ordinary Least Squares (OLS) method\n",
    "  * `regParam = 0`\n",
    "* Regularization by  Ridge regression\n",
    "  * `regParam > 0`, `elasticNetParam = 0` \n",
    "* Regularization by  LASSO method\n",
    "  * `regParam  > 0`, `elasticNetParam = 1` \n",
    "* Regularization by Elastic Net method\n",
    "  * `regParam > 0` , `1 > elasticNetParam > 0` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# TEST 1: Optimize regParam, elasticNet, maxIter\n",
    "'''\n",
    "subset = data_final.select(['label_q0','F1bANOVA']) \n",
    "\n",
    "train,test = subset.randomSplit([0.7,0.3])\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "modelname = 'linreg_optimized'\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr=LinearRegression(labelCol='label_q0',featuresCol='F1bANOVA')\n",
    "\n",
    "# Choose parameters to vary over test\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(lr.regParam, [0, 0.0001, 0.01, 0.1, 0.5, 1.0, 2.0])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.2, 0.5, 0.8, 1.0])\\\n",
    ".addGrid(lr.maxIter, [2, 4, 16, 32, 100])\\\n",
    ".build()\n",
    "\n",
    "evaluator=RegressionEvaluator(predictionCol='prediction',labelCol='label_q0',metricName='r2')\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "# Test the NLP model\n",
    "testresults = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49de83",
   "metadata": {},
   "source": [
    "* let's see which was the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8326efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mod = cvModel.bestModel\n",
    "param_dict = best_mod.extractParamMap() \n",
    "\n",
    "sane_dict = {}\n",
    "for k, v in param_dict.items():\n",
    "    sane_dict[k.name] = v\n",
    "\n",
    "best_reg = sane_dict[\"regParam\"]\n",
    "best_elastic_net = sane_dict[\"elasticNetParam\"]\n",
    "best_max_iter = sane_dict[\"maxIter\"]\n",
    "sane_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec34a2",
   "metadata": {},
   "source": [
    "* It appears that the best model was **LASSO** method of regularization: <br>\n",
    "{'aggregationDepth': 2,\n",
    " 'elasticNetParam': 1.0,\n",
    " 'epsilon': 1.35,\n",
    " 'featuresCol': 'F1bANOVA',\n",
    " 'fitIntercept': True,\n",
    " 'labelCol': 'label_q0',\n",
    " 'loss': 'squaredError',\n",
    " 'maxBlockSizeInMB': 0.0,\n",
    " 'maxIter': 100,\n",
    " 'predictionCol': 'prediction',\n",
    " 'regParam': 0.1,\n",
    " 'solver': 'auto',\n",
    " 'standardization': True,\n",
    " 'tol': 1e-06}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fcd3d",
   "metadata": {},
   "source": [
    "* let's see how it compares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8e380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RE RUN ML COMPARISON WITH NEW ANOVA FEATURES & SCALED FEATURES\n",
    "'''\n",
    "finalColumns = ['Name','label_q0','features2b','features2bSs','features2bSm','F1bANOVA','F2bANOVA','features1a','features1b','features3','features4a','features4b']\n",
    "from pyspark.ml.regression import (LinearRegression,\n",
    "                                   DecisionTreeRegressor,RandomForestRegressor,GBTRegressor,\n",
    "                                   GeneralizedLinearRegression,IsotonicRegression)\n",
    "\n",
    "featuresName = '' # temp value, redefined below\n",
    "\n",
    "#lr_df = pd.DataFrame()     # already exists from prior run\n",
    "labelName = 'label_q0'  # SPECIFY\n",
    "\n",
    "df = lr_df\n",
    "\n",
    "i = 1\n",
    "coefficients = {}\n",
    "allFeatures = finalColumns\n",
    "allFeatures.remove('Name')\n",
    "allFeatures.remove(labelName)\n",
    "\n",
    "for index,features in enumerate(allFeatures):\n",
    "    featnum = allFeatures\n",
    "    featuresName = features\n",
    "    \n",
    "    #modeltype = lr  # SPECIFY (lr,dtr,rfr,gbtr,glr,ir)\n",
    "    \n",
    "    # lr_opt2 (LASSO) was detected as bestmodel for optimizing r2\n",
    "    lr1_opt2 = LinearRegression(featuresCol=featuresName,labelCol='label_q0',regParam=0.1,elasticNetParam=1,maxIter=100)\n",
    "    \n",
    "    modeltype = lr1_opt2 #lr1\n",
    "    modeltypeVariantNo = 'lr1_opt2' # SPECIFY modeltype variation number\n",
    "    modelname = f\"{modeltypeVariantNo}_{featnum[index]}\" \n",
    "    \n",
    "    mymodel = modeltype.fit(train)\n",
    "    myresults = mymodel.transform(test)\n",
    "    \n",
    "    # CALCULATE KEY EVALS\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    regEvaluator = RegressionEvaluator(labelCol=labelName,predictionCol='prediction')\n",
    "    evalMetrics = {regEvaluator:['rmse','mse','mae','r2','var']}\n",
    "    evaluation2 = []\n",
    "    evaluator = regEvaluator\n",
    "    for each_metric in evalMetrics[evaluator]:        \n",
    "        metric = each_metric\n",
    "        \n",
    "        result = evaluator.evaluate(myresults, {evaluator.metricName: metric})\n",
    "        evaluation2.append((metric,result))\n",
    "        \n",
    "    r2_adj = mymodel.summary.r2adj\n",
    "    evaluation2.append(('r2_adj(Training)',r2_adj))\n",
    "    column0 = [x for x,y in evaluation2]\n",
    "    column1 = [y for x,y in evaluation2]\n",
    "    lr_df['metric'] = column0\n",
    "    lr_df[modelname] = column1\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "\n",
    "testx = lr_df.set_index('metric')\n",
    "testx = testx.drop(columns='lr1_features2bSm')\n",
    "testx = testx.transpose()\n",
    "testx.sort_values(by=['r2'],ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
