{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSM VM config prep\n",
    "import findspark\n",
    "findspark.init('/home/mitch/spark-3.3.0-bin-hadoop2')\n",
    "import pyspark\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BApredsV2').getOrCreate()\n",
    "\n",
    "# --- suppress future spark warnings/error/etc output ---\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bfa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data_and_merge():\n",
    "    labels_and_calcs = spark.read.csv(\"bioavailability_data_final.csv\",inferSchema=True,sep=',',header=True)\n",
    "    df1 = labels_and_calcs.toPandas()\n",
    "\n",
    "    df2 = pd.read_pickle('bioavailabilityData_w_Frags_simpler.pkl')\n",
    "    df2 = df2.drop(columns=['drug_smiles','ba_pct'])\n",
    "\n",
    "    data = pd.merge(df1,df2,how='left',left_on='_c0',right_on=df2.index)\n",
    "    data = spark.createDataFrame(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load_data_and_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "# INTIAL LABELS:\n",
    "# --- Data has 1 continuous label column, and 4 categorical label columns (discretized variants of continuous label).\n",
    "# ------ categorical labels applied by dividing the continuous label values into 3-5 categories \n",
    "# ------ the value range associated with each group were selected based on histogram dist./mean/stdev\n",
    "# --- We'll add one more discretization variant,  using Spark's built-in QuantileDiscretizer\n",
    "'''\n",
    "# -- Add QuantileDiscretizer labels\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "import pandas as pd\n",
    "qd5 = QuantileDiscretizer(numBuckets=5,inputCol='BA_pct',outputCol='label_QD5')\n",
    "\n",
    "data_wLabels = qd5.fit(data).transform(data)\n",
    "\n",
    "# -- INDEX / ENCODE LABELS\n",
    "from pyspark.ml.feature import (StringIndexer,OneHotEncoder)\n",
    "\n",
    "label_quant0 = 'BA_pct'\n",
    "label_cat0_vector = OneHotEncoder(inputCol='label_QD5',outputCol='label_cat0_vector')\n",
    "\n",
    "label_cat1_index = StringIndexer(inputCol='label1',outputCol='label_cat1_index')\n",
    "label_cat1_vector = OneHotEncoder(inputCol='label_cat1_index',outputCol='label_cat1_vector')\n",
    "\n",
    "label_cat2_index = StringIndexer(inputCol='label2',outputCol='label_cat2_index')\n",
    "label_cat2_vector = OneHotEncoder(inputCol='label_cat2_index',outputCol='label_cat2_vector')\n",
    "\n",
    "label_cat3_index = StringIndexer(inputCol='label3a',outputCol='label_cat3_index')\n",
    "label_cat3_vector = OneHotEncoder(inputCol='label_cat3_index',outputCol='label_cat3_vector')\n",
    "\n",
    "label_cat4_index = StringIndexer(inputCol='label3b',outputCol='label_cat4_index')\n",
    "label_cat4_vector = OneHotEncoder(inputCol='label_cat4_index',outputCol='label_cat4_vector')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "label_pipeline = Pipeline(stages=[label_cat0_vector,\n",
    "                                 label_cat1_index,label_cat1_vector,\n",
    "                                 label_cat2_index,label_cat2_vector,\n",
    "                                 label_cat3_index,label_cat3_vector,\n",
    "                                 label_cat4_index,label_cat4_vector])\n",
    "data_wLabels = label_pipeline.fit(data_wLabels).transform(data_wLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391d85b",
   "metadata": {},
   "source": [
    "## Test predictions by fragment data alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bf93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = data_wLabels.select(['Name','BA_pct','MolWt','MolLogP','TPSA',\n",
    "                                   'label_QD5','label1','label2','label3a','label3b',\n",
    "                                   'label_cat0_vector','label_cat1_vector','label_cat2_vector','label_cat3_vector','label_cat4_vector',\n",
    "                                  'frags_all','frags_better','frags_best','frags_efgs','frags_brics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram,Word2Vec,CountVectorizer,HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # Fragments NLP processing  - NEW -\n",
    "'''\n",
    "\n",
    "from pyspark.ml.feature import NGram,Word2Vec,CountVectorizer,HashingTF,IDF\n",
    "\n",
    "fragment_types = ['frags_all','frags_better','frags_best','frags_efgs','frags_brics']\n",
    "test_subset_nlp = test_subset\n",
    "for frag_type in fragment_types:\n",
    "    \n",
    "    \n",
    "    #output_name_level1 = f\"nlp_{frag_type}\"\n",
    "    frag_type_short = frag_type.replace('frags_','')\n",
    "    \n",
    "    cv = CountVectorizer(inputCol=frag_type, outputCol=f\"{frag_type_short}_cv\", minDF=2.0)\n",
    "    cv_idf = IDF(inputCol=f\"{frag_type_short}_cv\", outputCol=f\"{frag_type_short}_cv_idf\")\n",
    "    \n",
    "    w2v = Word2Vec(inputCol=frag_type, outputCol=f\"{frag_type_short}_w2v\")\n",
    "    \n",
    "    n2gram = NGram(n=2, inputCol=frag_type, outputCol=f\"{frag_type_short}_n2g\")\n",
    "    n2gram_cv = CountVectorizer(inputCol=f\"{frag_type_short}_n2g\", outputCol=f\"{frag_type_short}_n2g_cv\", minDF=2.0)\n",
    "    n2gram_cv_idf = IDF(inputCol=f\"{frag_type_short}_n2g_cv\", outputCol=f\"{frag_type_short}_n2g_cv_idf\")\n",
    "    \n",
    "    \n",
    "    nlp_pipeline = Pipeline(stages=[cv,cv_idf,w2v,n2gram,n2gram_cv,n2gram_cv_idf])\n",
    "    test_subset_nlp = nlp_pipeline.fit(test_subset_nlp).transform(test_subset_nlp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77449922",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_nlp = test_subset_nlp.select(['Name','BA_pct','MolWt','MolLogP','TPSA',#'label_QD5','label1','label2','label3a','label3b','label_cat0_vector','label_cat1_vector','label_cat2_vector','label_cat3_vector','label_cat4_vector',\n",
    "                                      'all_cv', 'all_cv_idf', 'all_w2v', 'all_n2g_cv', 'all_n2g_cv_idf', \n",
    "                                      'better_cv', 'better_cv_idf', 'better_w2v', 'better_n2g_cv', 'better_n2g_cv_idf', \n",
    "                                      'best_cv', 'best_cv_idf', 'best_w2v', 'best_n2g_cv', 'best_n2g_cv_idf', \n",
    "                                      'efgs_cv', 'efgs_cv_idf', 'efgs_w2v', 'efgs_n2g_cv', 'efgs_n2g_cv_idf', \n",
    "                                      'brics_cv', 'brics_cv_idf', 'brics_w2v', 'brics_n2g_cv', 'brics_n2g_cv_idf'])\n",
    "\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer)\n",
    "\n",
    "vector_assemblers = []\n",
    "\n",
    "alternative_features = ['all_cv', 'all_cv_idf', 'all_w2v', 'all_n2g_cv', 'all_n2g_cv_idf', \n",
    "                      'better_cv', 'better_cv_idf', 'better_w2v', 'better_n2g_cv', 'better_n2g_cv_idf', \n",
    "                      'best_cv', 'best_cv_idf', 'best_w2v', 'best_n2g_cv', 'best_n2g_cv_idf', \n",
    "                      'efgs_cv', 'efgs_cv_idf', 'efgs_w2v', 'efgs_n2g_cv', 'efgs_n2g_cv_idf', \n",
    "                      'brics_cv', 'brics_cv_idf', 'brics_w2v', 'brics_n2g_cv', 'brics_n2g_cv_idf']\n",
    "output_features = \"\"\n",
    "for feats in alternative_features:\n",
    "    #feats_input = ['MolWt','MolLogP','TPSA',feats]\n",
    "    feats_input = [feats]\n",
    "    feats_output = f\"FEAT_{feats}\"\n",
    "    \n",
    "    vec_assembler = VectorAssembler(inputCols=feats_input, outputCol=feats_output)\n",
    "    \n",
    "    vector_assemblers.append(vec_assembler)\n",
    "    \n",
    "    output_features += \"'\"+feats_output+\"'\"+\", \"\n",
    "output_features = output_features[0:len(output_features)-2]\n",
    "print(output_features)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "feature_pipeline = Pipeline(stages=[x for x in vector_assemblers])\n",
    "\n",
    "test_subset_nlpFeatures = feature_pipeline.fit(test_subset_nlp).transform(test_subset_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset_final = test_subset_nlpFeatures.select(['Name','BA_pct',\n",
    "                        'all_cv', 'all_cv_idf', 'all_w2v', 'all_n2g_cv', 'all_n2g_cv_idf', \n",
    "                      'better_cv', 'better_cv_idf', 'better_w2v', 'better_n2g_cv', 'better_n2g_cv_idf', \n",
    "                      'best_cv', 'best_cv_idf', 'best_w2v', 'best_n2g_cv', 'best_n2g_cv_idf', \n",
    "                      'efgs_cv', 'efgs_cv_idf', 'efgs_w2v', 'efgs_n2g_cv', 'efgs_n2g_cv_idf', \n",
    "                      'brics_cv', 'brics_cv_idf', 'brics_w2v', 'brics_n2g_cv', 'brics_n2g_cv_idf'])\n",
    "\n",
    "(training,testing) = test_subset_final.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67487843",
   "metadata": {},
   "outputs": [],
   "source": [
    "allFeatures =  ['all_cv', 'all_cv_idf', 'all_w2v', 'all_n2g_cv', 'all_n2g_cv_idf', \n",
    "                      'better_cv', 'better_cv_idf', 'better_w2v', 'better_n2g_cv', 'better_n2g_cv_idf', \n",
    "                      'best_cv', 'best_cv_idf', 'best_w2v', 'best_n2g_cv', 'best_n2g_cv_idf', \n",
    "                      'efgs_cv', 'efgs_cv_idf', 'efgs_w2v', 'efgs_n2g_cv', 'efgs_n2g_cv_idf', \n",
    "                      'brics_cv', 'brics_cv_idf', 'brics_w2v', 'brics_n2g_cv', 'brics_n2g_cv_idf']\n",
    "featuresName = '' # temp value, redefined below\n",
    "\n",
    "eval_df = pd.DataFrame() # already exists from prior run\n",
    "labelName = 'BA_pct'  # SPECIFY\n",
    "\n",
    "modelname_short = 'rfr'\n",
    "iteration = 4\n",
    "\n",
    "''' \n",
    "# Load Evaluation History records file\n",
    "'''\n",
    "with open('evaluation_history.pickle', 'rb') as handle:\n",
    "    evaluation_history = pickle.load(handle)\n",
    "\n",
    "evaluation_history[modelname_short] = {}\n",
    "evaluation_history[modelname_short][iteration] = {}\n",
    "evaluation_history[modelname_short][iteration]['source'] = 'frags_all'\n",
    "evaluation_history[modelname_short][iteration]['label'] = labelName\n",
    "\n",
    "\n",
    "for index,features in enumerate(allFeatures):\n",
    "    featuresName = features\n",
    "    \n",
    "    '''# SPECIFY MODEL \n",
    "    '''\n",
    "    from pyspark.ml.regression import RandomForestRegressor\n",
    "    rfr = RandomForestRegressor(featuresCol=features,labelCol='BA_pct')\n",
    "    modeltype = rfr  # SPECIFY (lr,dtr,rfr,gbtr,glr,ir)\n",
    "    \n",
    "    \n",
    "    modelname = f\"rfr{iteration}_{allFeatures[index]}\"\n",
    "    modelname = modelname.replace('features_','')\n",
    "    \n",
    "    evaluation_history[modelname_short][iteration][features] = {}\n",
    "    \n",
    "    # FIT/TRAIN MODEL & TRANSFORM DATA\n",
    "    mymodel = modeltype.fit(training)\n",
    "    myresults = mymodel.transform(testing)\n",
    "    \n",
    "    # CALCULATE KEY EVALS\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    regEvaluator = RegressionEvaluator(labelCol=labelName,predictionCol='prediction')\n",
    "\n",
    "    evaluator = regEvaluator\n",
    "    evalMetrics = {regEvaluator:['rmse','mse','mae','r2','var']}\n",
    "    \n",
    "    evaluation = []\n",
    "    \n",
    "    for each_metric in evalMetrics[evaluator]:        \n",
    "        metric = each_metric\n",
    "\n",
    "        result = evaluator.evaluate(myresults, {evaluator.metricName: metric})\n",
    "\n",
    "        evaluation.append((metric,result))\n",
    "        \n",
    "        evaluation_history[modelname_short][iteration][features][metric] = result\n",
    "        \n",
    "    #r2_adj = mymodel.summary.r2adj\n",
    "    #evaluation.append(('r2_adj(Training)',r2_adj))\n",
    "    column0 = [x for x,y in evaluation]\n",
    "    column1 = [y for x,y in evaluation]\n",
    "    eval_df['metric'] = column0\n",
    "    eval_df[modelname] = column1\n",
    "    \n",
    "''' # BACKUP EVALUATION HISTORY\n",
    "'''\n",
    "import pickle\n",
    "with open('evaluation_history.pickle', 'wb') as handle:\n",
    "    pickle.dump(evaluation_history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 4\n",
    "pd.set_option('display.max_columns',None)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a320ac",
   "metadata": {},
   "source": [
    "### Results:\n",
    "**Findings:**\n",
    "1. the most effective fragment representation is `frags_all`\n",
    "2. the most effective nlp representation is **CountVectorizer** or CV-IDF > bigram CV > word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a390edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
