{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb689a",
   "metadata": {},
   "source": [
    "# ML Trial 01 \n",
    "### _Test predictive performance using all Property features_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100c4f9",
   "metadata": {},
   "source": [
    "* specify spark environment for this pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606cd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSM VM config prep\n",
    "import findspark\n",
    "findspark.init('/home/mitch/spark-3.3.0-bin-hadoop2')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b840a44",
   "metadata": {},
   "source": [
    "* create a spark session & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013698ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BApredsV1').getOrCreate()\n",
    "\n",
    "# load the data\n",
    "data = spark.read.csv(\"bioavailability_data_edit02.csv\",inferSchema=True,sep=',',header=True)\n",
    "\n",
    "# --- suppress future spark warnings/error/etc output ---\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4609d1",
   "metadata": {},
   "source": [
    "* <font color='gray'> Regarding data labels, recall that: </font>\n",
    "  * <font color='gray'> The data has 5 labels: </font>\n",
    "      *  <font color='gray'> **1 continuous label:** the BA percentage (_the original label_) </font> \n",
    "      *  <font color='gray'>**4 categorical labels:** the discretized groups of BA percentage </font> <br>\n",
    "  * <font color='gray'> The categorical label columns were created by dividing _BA percentage_ into groups. The _\"bucketing\"/discretization ranges_ for these divisions were determined based on the review of histogram distributions and label statistics (_e.g.,_ grouped label mean & stdev) </font>\n",
    "<br><br>\n",
    "* for comparison with the manually-labeled categorical columns, an additional categorical label column will be added using Spark's built-in **QuantileDiscretizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff82b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "import pandas as pd\n",
    "qd5 = QuantileDiscretizer(numBuckets=5,inputCol='BA_pct',outputCol='label_QD5')\n",
    "data = qd5.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcb8a1",
   "metadata": {},
   "source": [
    "* <font color='blue'> _For a quick review of all data labels, let's display them in a grouped DF:_ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087797e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+--------+-------------+-----------+-----------+\n",
      "|label_QD5|  label3a|  label3b|   label2|  label1|count(BA_pct)|min(BA_pct)|max(BA_pct)|\n",
      "+---------+---------+---------+---------+--------+-------------+-----------+-----------+\n",
      "|      0.0| very low| very low| very low|     low|          309|        0.0|       13.0|\n",
      "|      1.0| very low| very low| very low|     low|           47|       14.0|       18.0|\n",
      "|      1.0| very low|      low| very low|     low|            4|       18.5|       19.0|\n",
      "|      1.0|      low|      low| very low|     low|           46|       19.5|       20.0|\n",
      "|      1.0|      low|      low|      low|     low|          125|       21.0|       33.0|\n",
      "|      1.0|      low|      low|      low|moderate|           45|       34.0|       38.0|\n",
      "|      1.0| moderate|      low|      low|moderate|            2|       39.0|       39.0|\n",
      "|      1.0| moderate| moderate|      low|moderate|           52|       40.0|       43.0|\n",
      "|      2.0| moderate| moderate|      low|moderate|          165|       44.0|       50.0|\n",
      "|      2.0| moderate| moderate|     high|moderate|          113|       51.0|       61.0|\n",
      "|      2.0| moderate|     high|     high|moderate|            7|       62.0|       62.0|\n",
      "|      2.0|     high|     high|     high|moderate|           29|       62.5|       65.0|\n",
      "|      3.0|     high|     high|     high|moderate|            8|       66.0|       66.0|\n",
      "|      3.0|     high|     high|     high|    high|          275|       67.0|       80.0|\n",
      "|      3.0|     high|     high|very high|    high|            5|       81.0|       81.0|\n",
      "|      3.0|very high|     high|very high|    high|            8|       82.0|       82.0|\n",
      "|      3.0|very high|very high|very high|    high|           19|       82.5|       84.0|\n",
      "|      4.0|very high|very high|very high|    high|          329|       85.0|      100.0|\n",
      "+---------+---------+---------+---------+--------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' # FINAL LABELS:\n",
    "# ...1 continuous label column\n",
    "# ...5 categorical label columns\n",
    "# ---- label1 has 3 categories          -->                     low   /  mid   / high \n",
    "# ---- label2 has 4 categories          -->          very low / low       /      high / very high\n",
    "# ---- the rest have 5 categories       -->          very low / low / moderate / high / very high\n",
    "'''\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data.select(['BA_pct','label_QD5','label1','label2','label3a','label3b'])\\\n",
    "                .groupby('label_QD5','label3a','label3b','label2','label1')\\\n",
    "                    .agg(F.count('BA_pct'),F.min('BA_pct'),F.max('BA_pct'))\\\n",
    "                        .orderBy('max(BA_pct)')\\\n",
    "                            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c8c1a",
   "metadata": {},
   "source": [
    "#### first regression and classification test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6d3f9",
   "metadata": {},
   "source": [
    "* prepare feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc1b0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_feature = ['MolWt','ExactMolWt','qed','MolLogP','MolMR','VSA_total','LabuteASA','TPSA',\n",
    "                   'MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge','MinAbsPartialCharge',\n",
    "                   'NumHAcceptors','NumHDonors','HeavyAtomCount','NumHeteroatoms','NumRotatableBonds',\n",
    "                   'NHOHCount','NOCount','FractionCSP3','RingCount','NumAliphaticRings','NumAromaticRings',\n",
    "                   'NumAliphaticHeterocycles','NumAromaticHeterocycles','NumSaturatedHeterocycles',\n",
    "                   'NumSaturatedRings','BalabanJ','BertzCT','HallKierAlpha',\n",
    "                   'fracVSA_PEOE01','fracVSA_PEOE02','fracVSA_PEOE03','fracVSA_PEOE04','fracVSA_PEOE05',\n",
    "                   'fracVSA_PEOE06','fracVSA_PEOE07','fracVSA_PEOE08','fracVSA_PEOE09','fracVSA_PEOE10',\n",
    "                   'fracVSA_PEOE11','fracVSA_PEOE12','fracVSA_PEOE13','fracVSA_PEOE14',\n",
    "                   'fracVSA_SMR01','fracVSA_SMR02','fracVSA_SMR03','fracVSA_SMR04','fracVSA_SMR05',\n",
    "                   'fracVSA_SMR06','fracVSA_SMR07','fracVSA_SMR08','fracVSA_SMR09','fracVSA_SMR10',\n",
    "                   'fracVSA_SlogP01','fracVSA_SlogP02','fracVSA_SlogP03','fracVSA_SlogP04',\n",
    "                   'fracVSA_SlogP05','fracVSA_SlogP06','fracVSA_SlogP07','fracVSA_SlogP08',\n",
    "                   'fracVSA_SlogP09','fracVSA_SlogP10','fracVSA_SlogP11','fracVSA_SlogP12']\n",
    "\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer)\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = cols_to_feature, outputCol='features')\n",
    "data_w_features = vec_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaac88c",
   "metadata": {},
   "source": [
    "* index/encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81a6e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (StringIndexer,OneHotEncoder)\n",
    "\n",
    "label_quant0 = 'BA_pct'\n",
    "\n",
    "label_cat0_vector = OneHotEncoder(inputCol='label_QD5',outputCol='label_cat0_vector')\n",
    "\n",
    "label_cat1_index = StringIndexer(inputCol='label1',outputCol='label_cat1_index')\n",
    "label_cat1_vector = OneHotEncoder(inputCol='label_cat1_index',outputCol='label_cat1_vector')\n",
    "\n",
    "label_cat2_index = StringIndexer(inputCol='label2',outputCol='label_cat2_index')\n",
    "label_cat2_vector = OneHotEncoder(inputCol='label_cat2_index',outputCol='label_cat2_vector')\n",
    "\n",
    "label_cat3_index = StringIndexer(inputCol='label3a',outputCol='label_cat3_index')\n",
    "label_cat3_vector = OneHotEncoder(inputCol='label_cat3_index',outputCol='label_cat3_vector')\n",
    "\n",
    "label_cat4_index = StringIndexer(inputCol='label3b',outputCol='label_cat4_index')\n",
    "label_cat4_vector = OneHotEncoder(inputCol='label_cat4_index',outputCol='label_cat4_vector')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "data_pipeline = Pipeline(stages=[label_cat0_vector,\n",
    "                                 label_cat1_index,label_cat1_vector,\n",
    "                                 label_cat2_index,label_cat2_vector,\n",
    "                                 label_cat3_index,label_cat3_vector,\n",
    "                                 label_cat4_index,label_cat4_vector])\n",
    "\n",
    "data_w_features = data_w_features.select(['Name','BA_pct','label_QD5','label1','label2','label3a','label3b','features'])\n",
    "data_prefinal = data_pipeline.fit(data_w_features).transform(data_w_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed03db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data\n",
    "data_prefinal2 = data_prefinal.withColumnRenamed('BA_pct','label_q0')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_QD5','label_cat0')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat1_index','label_cat1')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat2_index','label_cat2')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat3_index','label_cat3')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat4_index','label_cat4')\n",
    "\n",
    "data_final = data_prefinal2.select(['Name',\n",
    "                                    'label_q0',\n",
    "                                    'label_cat0','label_cat1',\n",
    "                                    'label_cat2','label_cat3','label_cat4',\n",
    "                                    'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42180862",
   "metadata": {},
   "source": [
    "* test a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe94a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.686537452357864 0.17132735316138348\n"
     ]
    }
   ],
   "source": [
    "subset_q0 = data_final.select(['label_q0','features'])\n",
    "train1_q0,test1_q0 = subset_q0.randomSplit([0.7,0.3])\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lm_A = LinearRegression(featuresCol='features',labelCol='label_q0',predictionCol='prediction')\n",
    "\n",
    "lmModel_1A = lm_A.fit(train1_q0)\n",
    "lmResults1A = lmModel_1A.evaluate(test1_q0)\n",
    "\n",
    "print(lmResults1A.rootMeanSquaredError, lmResults1A.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0be50",
   "metadata": {},
   "source": [
    "<font color='purple'> ***Observations:*** </font> <br>\n",
    "The r-squared value of the Linear Regression model is only 0.171, which is rather disappointing.\n",
    "<br>\n",
    "<font color='orange'> ***Next step:*** </font> <br>\n",
    "Let's also test the performance of a Classification model, to predict the categorical label columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f56c0",
   "metadata": {},
   "source": [
    "* test a logistic regression model using `label_cat0` (Spark's QuantileDiscretizer label column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4108cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_test(dataset,modelname,labelName,featuresName,eval_comparison):\n",
    "    ''' # Select and split data \n",
    "    '''\n",
    "    subset = dataset.select([labelName,featuresName])\n",
    "    train,test = subset.randomSplit([0.7,0.3])\n",
    "\n",
    "    ''' # Instantiate and run model \n",
    "    '''\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    lr = LogisticRegression(featuresCol=featuresName,labelCol=labelName,predictionCol='prediction')\n",
    "\n",
    "    mymodel = lr.fit(train)\n",
    "    myresults = mymodel.transform(test)\n",
    "\n",
    "    ''' # Evaluate results on multiple metrics, output to df\n",
    "    '''\n",
    "    datasetName = myresults\n",
    "\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "    multiEvaluator = MulticlassClassificationEvaluator(labelCol=labelName, predictionCol=\"prediction\")\n",
    "    binEvaluator = BinaryClassificationEvaluator(labelCol=labelName, rawPredictionCol=\"prediction\")\n",
    "\n",
    "    evalMetrics = {binEvaluator:['areaUnderROC','areaUnderPR'], \n",
    "                   multiEvaluator:['f1','weightedPrecision','weightedRecall','accuracy']}\n",
    "    evaluation = []\n",
    "    for each_evaluator in [binEvaluator,multiEvaluator]:\n",
    "        evaluator = each_evaluator\n",
    "        for each_metric in evalMetrics[evaluator]:        \n",
    "            metric = each_metric\n",
    "            result = evaluator.evaluate(datasetName, {evaluator.metricName: metric})\n",
    "            evaluation.append((metric,result))\n",
    "\n",
    "    column0 = [x for x,y in evaluation]\n",
    "    column1 = [y for x,y in evaluation]\n",
    "    eval_comparison['metric'] = column0\n",
    "    eval_comparison[modelname] = column1\n",
    "\n",
    "    return eval_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95e73190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>lr_cat0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>areaUnderROC</td>\n",
       "      <td>0.664308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>areaUnderPR</td>\n",
       "      <td>0.834057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.337569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weightedPrecision</td>\n",
       "      <td>0.343967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weightedRecall</td>\n",
       "      <td>0.334773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.334773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              metric   lr_cat0\n",
       "0       areaUnderROC  0.664308\n",
       "1        areaUnderPR  0.834057\n",
       "2                 f1  0.337569\n",
       "3  weightedPrecision  0.343967\n",
       "4     weightedRecall  0.334773\n",
       "5           accuracy  0.334773"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_comparison = pd.DataFrame()\n",
    "dataset = data_final\n",
    "featuresName = 'features'\n",
    "labelName = 'label_cat0'\n",
    "modelname = 'lr_cat0'\n",
    "\n",
    "eval_comparison = log_reg_test(dataset,modelname,labelName,featuresName,eval_comparison)\n",
    "\n",
    "eval_comparison.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c997e",
   "metadata": {},
   "source": [
    "<font color='purple'> ***Observations:*** </font> <br>\n",
    "Logistic Regression shows 33.5% accuracy when predicting BA as either: _very low, low, mid, high, very high_\n",
    "<br>\n",
    "<font color='orange'> ***Next step:*** </font> <br>\n",
    "Check performance when predicting between a 3-class label column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9f850",
   "metadata": {},
   "source": [
    "* test a logistic regression model using `label_cat1` (3-category BA labels: _low, medium, high_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b1e7405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>lr_cat0</th>\n",
       "      <th>lr_cat1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>areaUnderROC</td>\n",
       "      <td>0.664308</td>\n",
       "      <td>0.673774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>areaUnderPR</td>\n",
       "      <td>0.834057</td>\n",
       "      <td>0.686729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.337569</td>\n",
       "      <td>0.527839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weightedPrecision</td>\n",
       "      <td>0.343967</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weightedRecall</td>\n",
       "      <td>0.334773</td>\n",
       "      <td>0.548980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.334773</td>\n",
       "      <td>0.548980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              metric   lr_cat0   lr_cat1\n",
       "0       areaUnderROC  0.664308  0.673774\n",
       "1        areaUnderPR  0.834057  0.686729\n",
       "2                 f1  0.337569  0.527839\n",
       "3  weightedPrecision  0.343967  0.535000\n",
       "4     weightedRecall  0.334773  0.548980\n",
       "5           accuracy  0.334773  0.548980"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_final\n",
    "featuresName = 'features'\n",
    "labelName = 'label_cat1'\n",
    "modelname = 'lr_cat1'\n",
    "\n",
    "eval_comparison = log_reg_test(dataset,modelname,labelName,featuresName,eval_comparison)\n",
    "\n",
    "eval_comparison.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080d7cd",
   "metadata": {},
   "source": [
    "<font color='purple'> ***Observations:*** </font> <br>\n",
    "Logistic Regression shows 54.9% accuracy when predicting BA between: _Low, Mid, High_\n",
    "<br>\n",
    "<font color='orange'> ***Next step:*** </font> <br>\n",
    "In the next notebook, we'll work on some feature engineering to see if we can improve predictive quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae7ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
