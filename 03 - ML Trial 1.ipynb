{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb689a",
   "metadata": {},
   "source": [
    "# ML Trial 01 \n",
    "### _Test predictive performance using all Property features_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100c4f9",
   "metadata": {},
   "source": [
    "* specify spark environment for this pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606cd755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSM VM config prep\n",
    "import findspark\n",
    "findspark.init('/home/mitch/spark-3.3.0-bin-hadoop2')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b840a44",
   "metadata": {},
   "source": [
    "* create a spark session & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "013698ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BApredsV1').getOrCreate()\n",
    "\n",
    "# --- suppress future spark warnings/error/etc output ---\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_pickle(\"data/bioavailability_data_wFeatures.pkl\")\n",
    "data = data.rename(columns={'Name':'name','_c0':'index'})\n",
    "data = data.drop(columns='drug_name')\n",
    "data = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c8c1a",
   "metadata": {},
   "source": [
    "#### first regression and classification test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6d3f9",
   "metadata": {},
   "source": [
    "* prepare feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc1b0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_feature = ['MolWt','ExactMolWt','qed','MolLogP','MolMR','VSA_total','LabuteASA','TPSA',\n",
    "                   'MaxPartialCharge','MinPartialCharge','MaxAbsPartialCharge','MinAbsPartialCharge',\n",
    "                   'NumHAcceptors','NumHDonors','HeavyAtomCount','NumHeteroatoms','NumRotatableBonds',\n",
    "                   'NHOHCount','NOCount','FractionCSP3','RingCount','NumAliphaticRings','NumAromaticRings',\n",
    "                   'NumAliphaticHeterocycles','NumAromaticHeterocycles','NumSaturatedHeterocycles',\n",
    "                   'NumSaturatedRings','BalabanJ','BertzCT','HallKierAlpha',\n",
    "                   'fracVSA_PEOE01','fracVSA_PEOE02','fracVSA_PEOE03','fracVSA_PEOE04','fracVSA_PEOE05',\n",
    "                   'fracVSA_PEOE06','fracVSA_PEOE07','fracVSA_PEOE08','fracVSA_PEOE09','fracVSA_PEOE10',\n",
    "                   'fracVSA_PEOE11','fracVSA_PEOE12','fracVSA_PEOE13','fracVSA_PEOE14',\n",
    "                   'fracVSA_SMR01','fracVSA_SMR02','fracVSA_SMR03','fracVSA_SMR04','fracVSA_SMR05',\n",
    "                   'fracVSA_SMR06','fracVSA_SMR07','fracVSA_SMR08','fracVSA_SMR09','fracVSA_SMR10',\n",
    "                   'fracVSA_SlogP01','fracVSA_SlogP02','fracVSA_SlogP03','fracVSA_SlogP04',\n",
    "                   'fracVSA_SlogP05','fracVSA_SlogP06','fracVSA_SlogP07','fracVSA_SlogP08',\n",
    "                   'fracVSA_SlogP09','fracVSA_SlogP10','fracVSA_SlogP11','fracVSA_SlogP12']\n",
    "\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer)\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = cols_to_feature, outputCol='features')\n",
    "data_w_features = vec_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaac88c",
   "metadata": {},
   "source": [
    "* index/encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a6e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import (StringIndexer,OneHotEncoder)\n",
    "\n",
    "label_quant0 = 'BA_pct'\n",
    "\n",
    "label_cat1_index = StringIndexer(inputCol='label1',outputCol='label_cat1_index')\n",
    "\n",
    "label_cat2_index = StringIndexer(inputCol='label2',outputCol='label_cat2_index')\n",
    "\n",
    "label_cat3_index = StringIndexer(inputCol='label3a',outputCol='label_cat3_index')\n",
    "\n",
    "label_cat4_index = StringIndexer(inputCol='label3b',outputCol='label_cat4_index')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "data_pipeline = Pipeline(stages=[label_cat1_index,\n",
    "                                 label_cat2_index,\n",
    "                                 label_cat3_index,\n",
    "                                 label_cat4_index])\n",
    "\n",
    "data_w_features = data_w_features.select(['Name','BA_pct','label_QD5','label1','label2','label3a','label3b','features'])\n",
    "data_prefinal = data_pipeline.fit(data_w_features).transform(data_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed03db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data\n",
    "data_prefinal2 = data_prefinal.withColumnRenamed('BA_pct','label_q0')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_QD5','label_cat0')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat1_index','label_cat1')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat2_index','label_cat2')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat3_index','label_cat3')\n",
    "data_prefinal2 = data_prefinal2.withColumnRenamed('label_cat4_index','label_cat4')\n",
    "\n",
    "data_final = data_prefinal2.select(['Name',\n",
    "                                    'label_q0',\n",
    "                                    'label_cat0','label_cat1',\n",
    "                                    'label_cat2','label_cat3','label_cat4',\n",
    "                                    'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42180862",
   "metadata": {},
   "source": [
    "* test a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe94a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.792385189676 0.21404417800257503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "subset_q0 = data_final.select(['label_q0','features'])\n",
    "train1_q0,test1_q0 = subset_q0.randomSplit([0.7,0.3])\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lm_A = LinearRegression(featuresCol='features',labelCol='label_q0',predictionCol='prediction')\n",
    "\n",
    "lmModel_1A = lm_A.fit(train1_q0)\n",
    "lmResults1A = lmModel_1A.evaluate(test1_q0)\n",
    "\n",
    "print(lmResults1A.rootMeanSquaredError, lmResults1A.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d0be50",
   "metadata": {},
   "source": [
    "<font color='purple'> ***Observations:*** </font> <br>\n",
    "The r-squared value of the Linear Regression model isn't great, which is rather disappointing.\n",
    "<br>\n",
    "<font color='orange'> ***Next step:*** </font> <br>\n",
    "Let's also test the performance of a Classification model, to predict the categorical label columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389f56c0",
   "metadata": {},
   "source": [
    "* test a logistic regression model using `label_cat0` (Spark's QuantileDiscretizer label column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4108cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_test(dataset,modelname,labelName,featuresName,eval_comparison):\n",
    "    ''' # Select and split data \n",
    "    '''\n",
    "    subset = dataset.select([labelName,featuresName])\n",
    "    train,test = subset.randomSplit([0.7,0.3])\n",
    "\n",
    "    ''' # Instantiate and run model \n",
    "    '''\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    lr = LogisticRegression(featuresCol=featuresName,labelCol=labelName,predictionCol='prediction')\n",
    "\n",
    "    mymodel = lr.fit(train)\n",
    "    myresults = mymodel.transform(test)\n",
    "\n",
    "    ''' # Evaluate results on multiple metrics, output to df\n",
    "    '''\n",
    "    datasetName = myresults\n",
    "\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "    multiEvaluator = MulticlassClassificationEvaluator(labelCol=labelName, predictionCol=\"prediction\")\n",
    "    binEvaluator = BinaryClassificationEvaluator(labelCol=labelName, rawPredictionCol=\"prediction\")\n",
    "\n",
    "    evalMetrics = {binEvaluator:['areaUnderROC','areaUnderPR'], \n",
    "                   multiEvaluator:['f1','weightedPrecision','weightedRecall','accuracy']}\n",
    "    evaluation = []\n",
    "    for each_evaluator in [binEvaluator,multiEvaluator]:\n",
    "        evaluator = each_evaluator\n",
    "        for each_metric in evalMetrics[evaluator]:        \n",
    "            metric = each_metric\n",
    "            result = evaluator.evaluate(datasetName, {evaluator.metricName: metric})\n",
    "            evaluation.append((metric,result))\n",
    "\n",
    "    column0 = [x for x,y in evaluation]\n",
    "    column1 = [y for x,y in evaluation]\n",
    "    eval_comparison['metric'] = column0\n",
    "    eval_comparison[modelname] = column1\n",
    "\n",
    "    return eval_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95e73190",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_reg_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m labelName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_cat0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m modelname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_cat0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m eval_comparison \u001b[38;5;241m=\u001b[39m \u001b[43mlog_reg_test\u001b[49m(dataset,modelname,labelName,featuresName,eval_comparison)\n\u001b[1;32m      9\u001b[0m eval_comparison\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_reg_test' is not defined"
     ]
    }
   ],
   "source": [
    "eval_comparison = pd.DataFrame()\n",
    "dataset = data_final\n",
    "featuresName = 'features'\n",
    "labelName = 'label_cat0'\n",
    "modelname = 'lr_cat0'\n",
    "\n",
    "eval_comparison = log_reg_test(dataset,modelname,labelName,featuresName,eval_comparison)\n",
    "\n",
    "eval_comparison.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c997e",
   "metadata": {},
   "source": [
    "<font color='purple'> ***Observations:*** </font> <br>\n",
    "Logistic Regression shows 33.5% accuracy when predicting BA as either: _very low, low, mid, high, very high_\n",
    "<br>\n",
    "<font color='orange'> ***Next step:*** </font> <br>\n",
    "Check performance when predicting between a 3-class label column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9f850",
   "metadata": {},
   "source": [
    "* test a logistic regression model using `label_cat1` (3-category BA labels: _low, medium, high_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b1e7405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>lr_cat0</th>\n",
       "      <th>lr_cat1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>areaUnderROC</td>\n",
       "      <td>0.664308</td>\n",
       "      <td>0.673774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>areaUnderPR</td>\n",
       "      <td>0.834057</td>\n",
       "      <td>0.686729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.337569</td>\n",
       "      <td>0.527839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weightedPrecision</td>\n",
       "      <td>0.343967</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weightedRecall</td>\n",
       "      <td>0.334773</td>\n",
       "      <td>0.548980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.334773</td>\n",
       "      <td>0.548980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              metric   lr_cat0   lr_cat1\n",
       "0       areaUnderROC  0.664308  0.673774\n",
       "1        areaUnderPR  0.834057  0.686729\n",
       "2                 f1  0.337569  0.527839\n",
       "3  weightedPrecision  0.343967  0.535000\n",
       "4     weightedRecall  0.334773  0.548980\n",
       "5           accuracy  0.334773  0.548980"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data_final\n",
    "featuresName = 'features'\n",
    "labelName = 'label_cat1'\n",
    "modelname = 'lr_cat1'\n",
    "\n",
    "eval_comparison = log_reg_test(dataset,modelname,labelName,featuresName,eval_comparison)\n",
    "\n",
    "eval_comparison.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080d7cd",
   "metadata": {},
   "source": [
    "<font color='purple'> ***Observations:*** </font> <br>\n",
    "Logistic Regression shows 54.9% accuracy when predicting BA between: _Low, Mid, High_\n",
    "<br>\n",
    "<font color='orange'> ***Next step:*** </font> <br>\n",
    "In the next notebook, we'll work on some feature engineering to see if we can improve predictive quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ae7ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
